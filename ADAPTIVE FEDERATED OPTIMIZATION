연합 학습(Federated learning):  중앙 서버가 있고 각 클라이언트가 있으면 중앙 서버에서 최신 모델 을 각 클라이언트에게 보내 
각 클라이언트가 그 최신모델을 바탕으로 각자 자기가 가지고 있는 데이터로 학습 후 다시 중앙서버에 보냄.
그리고 중앙서버가 그걸 집계해서 새로운 최신모델을 만듬. 그걸 여러번 반복하는것.(중앙 서버에 모델 보낼때 데이터를 보내는것이 아님.)
FEDAVG:연합 학습에서 제일 많이 쓰는 알고리즘으로 통신 비용(중앙서버랑 각 클라이언트가 주고 받는 )
그 비용을 줄임. 모든 클라이언트에 대해서가 아니라 매 라운드마다 임의로 클라이언트 선정 후 각 클라이언트가 
자신의 데이터로 여러번 학습한 후 중앙서버에 전달. 중앙서버는 선택된 클라이언트로부터 모델들을 받아서 최신 모델을 갱신 이걸 목표치가 되거나(어떤 특정값에 수렴) 통신 라운드 한계치까지
반복하게 됌.
근데 이게 잘안됌. -> 각 클라이언트간 데이터 이질성 문제 등 
적응형 최적화 같은 기법을 사용하면 해결 됌. -> 그래서 연합학습에 이걸 적용할거임. ->결론은 성능 향상.


++참고
SGD: 일반적인 경사하강법에 미니배치+업데이트에 학습률을 넣어 좀더 빠르게 수렴할 수 있도록 하는 개선된 경사하강법. 
-> 효율적 학습위한건데 이게 최적의 학습률 찾는게 어렵다 보니 ->> 적응적 학습률 쓰는 ADAGRAD,ADAM이런애들 써서 연합 학습에 적용시켜보겠다 이거.
 Client drift:각 클라이언트(사용자 기기)의 로컬 모델 업데이트가 중앙 서버의 글로벌 모델과 크게 달라지는 현상


